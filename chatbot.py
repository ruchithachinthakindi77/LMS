# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13dVVjhWQgpSy3rIJosvbScabHIAk_7TO
"""

import nltk
import re
from nltk.chat.util import Chat, reflections

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

import nltk
from nltk.chat.util import Chat, reflections
pairs = [
    (r"hi|hello|hey", ["Hello!", "Hi there!", "Hey!"]),
    (r"how are you?", ["I'm good, thanks!", "Doing well, what about you?"]),
    (r"what is your name?", ["I'm a chatbot!", "Call me ChatBot."]),
    (r"quit", ["Bye! Have a great day!"])
]
chatbot = Chat(pairs, reflections)
def chat_with_bot():
    print("Hi, I'm your chatbot. Type 'quit' to exit.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            print("Chatbot: Bye! Have a great day!")
            break
        response = chatbot.respond(user_input)
        print(f"Chatbot: {response}")

chat_with_bot()

context ="""The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Pari, France.
It is named after the engineer Gustave Eiffel, whose company designed and built the tower."""
question = "Who designed the Eiffel Tower?"
inputs = tokenizer(question, context, return_tensors="pt")
with torch.no_grad():
  outputs = model(**inputs)
  start_scores = outputs.start_logits
  end_scores = outputs.end_logits

from transformers import BertTokenizer, BertForQuestionAnswering
import torch

# Load the tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')

# Provide context and question
context = "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower."
question = "Who designed the Eiffel Tower?"

# Tokenize the inputs
inputs = tokenizer(question, context, return_tensors="pt")

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

# Print the results
print(outputs)

from transformers import BertTokenizer, BertForQuestionAnswering
import torch

# Load the tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')

# Provide context and question
context = """
The coronavirus (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus. Most people infected with the virus will experience mild to moderate respiratory illness and recover without requiring special treatment. However, some will become seriously ill and require medical attention. Older people and those with underlying medical conditions are more likely to develop serious illness. The best way to prevent and slow down transmission is to be well informed about the disease and how the virus spreads.
"""

question = "What is the coronavirus?"

# Tokenize the inputs
inputs = tokenizer(question, context, return_tensors="pt")

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

# Print the results
print(outputs)

